{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ciph3r007/Bengali-OCR/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ucKBB-xx9WL",
        "outputId": "dcd50350-0f16-4c51-a6cc-4d47fb52e80c"
      },
      "source": [
        "# Install JAX.\n",
        "!pip install --upgrade jax\n",
        "!pip install --upgrade jaxlib\n",
        "!pip install --upgrade trax\n",
        "\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: jax in /usr/local/lib/python3.7/dist-packages (0.2.12)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already up-to-date: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.65+cuda110)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jaxlib) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jaxlib) (1.15.0)\n",
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/51/305b839f51d53abb393777f743e497d27bb341478f3fdec4d6ddaccc9fb5/trax-1.3.7-py2.py3-none-any.whl (521kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 7.4MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.12)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.65+cuda110)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/e4/e2dc66207464795aafecc5c8cef9a35b5c9a61b974ac60a2c306c12bfd4c/t5-0.9.1-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 31.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.2)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.0)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/4b/01fe41784cf0edd8264fbc6fb90750fdaf365441492c425b6ce693c4122f/tfds_nightly-4.2.0.dev202104230108-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.8.1+cu101)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/10/37df0bc87ebf84e1414613176340e3aadc3697d2bd112bf63d3d4b1e848a/mesh_tensorflow-0.1.19-py3-none-any.whl (366kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 52.0MB/s \n",
            "\u001b[?25hCollecting seqio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/21/8161f170208b4da94036aa74c6974caaeca82a9a634e80bf98e1a0cd6e10/seqio-0.0.3-py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 38.1MB/s \n",
            "\u001b[?25hCollecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5->trax) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n",
            "Installing collected packages: tensorflow-text, funcsigs, tfds-nightly, portalocker, sacrebleu, sentencepiece, mesh-tensorflow, seqio, sacremoses, tokenizers, transformers, rouge-score, t5, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.19 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 seqio-0.0.3 t5-0.9.1 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202104230108 tokenizers-0.10.2 transformers-4.5.1 trax-1.3.7\n",
            "grpc://10.13.12.122:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmf07eJE8p9r"
      },
      "source": [
        "# Chatbot\n",
        "\n",
        "- [1:   Dataset](#1)\n",
        "- [2:   Preprocessing](#2)\n",
        "    - [2.1:   Creating input pipeline](#2.1)\n",
        "- [3:   Model Training](#4)\n",
        "- [4:   Testing](#5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTPBColehpV"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "# 1. The MultiWoz dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUiqp2FehpW"
      },
      "source": [
        "Installation and importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfY2FUYMe7UB",
        "outputId": "1800fcb9-84e3-42b0-ff00-47757ddcb245"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVYSzgookQH5",
        "outputId": "662f64b7-3935-4b02-fdc8-04518c3a8ff3"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/colab_data/chatbot/\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab_data/chatbot\n",
            "cbot.jpg  model        Reformer.jpg\tReversibleDecoder.png\n",
            "data\t  __pycache__  reversible2.PNG\tw4_unittest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4zpTnSVFIp",
        "outputId": "839e1678-aedd-4a87-b731-37f0d5beba08"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "!pip list | grep trax"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYblWc8YvAN"
      },
      "source": [
        "Dataset INFO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H8pB_yI8p-g",
        "outputId": "d7130ce5-1f63-4805-d37c-ae25950ba9ec"
      },
      "source": [
        "with open('data/README') as file:\n",
        "    print(file.read())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "#####################################################\n",
            "#  Copyright Cambridge Dialogue Systems Group, 2018 #\n",
            "#####################################################\n",
            "#####################################################\n",
            "\n",
            "Dataset contains the following files:\n",
            "1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have \"MUL\" in their names. Single domain dialogues have either \"SNG\" or \"WOZ\" in their names.\n",
            "2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.\n",
            "3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.\n",
            "4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.\n",
            "5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.\n",
            "6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.\n",
            "7. police_db.json: the Cambridge police station information.\n",
            "8. taxi_db.json: slot-value list for taxi domain.\n",
            "9. valListFile.txt: list of dialogues for validation.\n",
            "10. testListFile.txt: list of dialogues for testing.\n",
            "11. system_acts.json:\n",
            "  There are 6 domains ('Booking', 'Restaurant', 'Hotel', 'Attraction', 'Taxi', 'Train') and 1 dummy domain ('general').\n",
            "  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. 'Hotel-inform' means it is an 'inform' act in the Hotel domain.\n",
            "  Dialogue acts which cannot take slots, e.g., 'good bye', are defined under the 'general' domain.\n",
            "  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.\n",
            "  If a dialogue act takes no slots, e.g., dialogue act 'offer booking' for an utterance 'would you like to take a reservation?', its slot-value pair is ['none', 'none']\n",
            "  There are four types of values:\n",
            "  1) If a slot takes a binary value, e.g., 'has Internet' or 'has park', the value is either 'yes' or 'no'.\n",
            "  2) If a slot is under the act 'request', e.g., 'request' about 'area', the value is expressed as '?'.\n",
            "  3) The value that appears in the utterance e.g., the name of a restaurant.\n",
            "  4) If for some reason the turn does not have an annotation then it is labeled as \"No Annotation.\"\n",
            "12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.\n",
            "13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.\n",
            "14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpB3VlRBehpX"
      },
      "source": [
        "Declaring some CONSTANTS to be used later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUBmsdbehpX"
      },
      "source": [
        "DATA_FILE = 'data.json'\n",
        "DATA_DIR = './data'\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "VOCAB_DIR = 'data/vocabs'\n",
        "\n",
        "N_LAYERS = 6\n",
        "TRAIN_STEPS = 500\n",
        "LOAD_MODEL = False\n",
        "TRAIN = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yskv4W_ehpX"
      },
      "source": [
        "Loading the MultiWoz dataset from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K58I5vFB7GlP"
      },
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "    \n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGBnUfEk8p9x",
        "outputId": "33afe9cf-3bc6-4243-9553-d8c1de16225d"
      },
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMdkaGQrehpY"
      },
      "source": [
        "The dialogues are composed of multiple files and the filenames are used as keys in the dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKmwt7_zehpY",
        "outputId": "27369825-0e2f-48dd-804b-18a5e41369da"
      },
      "source": [
        "print(list(DIALOGUE_DB.keys())[0:7]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'PMUL1635.json', 'MUL2168.json', 'SNG0073.json', 'SNG01445.json', 'MUL2105.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KYeQLnG8p96",
        "outputId": "17c62510-2a06-40c0-8d96-4f2ca442ebc7"
      },
      "source": [
        "# get keys of the fifth file in the list above\n",
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['goal', 'log'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gj5aqF8p9_"
      },
      "source": [
        "Here `goal` points to a dictionary containing several key objectives of the conversation. `log` (a list) on the other hand contains the dialog in each of its item's `text` key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPPWwQ2s8p9_",
        "outputId": "c7edcf58-9da6-46b5-8ca0-d92a7cda8a76"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attraction': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'police': {},\n",
              " 'restaurant': {},\n",
              " 'taxi': {'fail_info': {},\n",
              "  'info': {'departure': \"saint john's college\",\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'leaveAt': '17:15'},\n",
              "  'reqt': ['car type', 'phone']},\n",
              " 'train': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq70g950ehpa",
        "outputId": "8742d7f4-43b1-4bff-c43b-5a82c3e53259"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metadata': {},\n",
              " 'text': \"I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Mohcu5ehpa"
      },
      "source": [
        "The conversion goes between two persons back and forth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6JY0sUgehpa",
        "outputId": "f7fa4c25-6b0e-4658-fe31-a98e32e8874b"
      },
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1:  I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0BjxciRehpb"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    logs = data_db[file]['log']\n",
        "    \n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = logs[i]['text']\n",
        "        \n",
        "        if i % 2 == 0:\n",
        "            result += delimiter_1\n",
        "        else:\n",
        "            result += delimiter_2\n",
        "            \n",
        "        result += cur_log\n",
        "\n",
        "    return result"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugvx0noP8p-G",
        "outputId": "831bbe61-a71b-4279-850e-87a9ccbee0cf"
      },
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "print(conversation)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apiFKExehpc"
      },
      "source": [
        "Prettifier function using termcolor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlqWwiriehpc",
        "outputId": "cd6b70fb-4dad-4419-c1b7-fb90145953f7"
      },
      "source": [
        "def print_conversation(conversation):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    \n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "        \n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "            \n",
        "print_conversation(conversation)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[32mPerson 2: Okay, do you have a specific area you want to stay in? \u001b[0m\n",
            "\u001b[31mPerson 1: no, i just need to make sure it's cheap. oh, and i need parking \u001b[0m\n",
            "\u001b[32mPerson 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \u001b[0m\n",
            "\u001b[31mPerson 1: Yes, please. 6 people 3 nights starting on tuesday. \u001b[0m\n",
            "\u001b[32mPerson 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \u001b[0m\n",
            "\u001b[31mPerson 1: how about only 2 nights. \u001b[0m\n",
            "\u001b[32mPerson 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \u001b[0m\n",
            "\u001b[31mPerson 1: No, that will be all. Good bye. \u001b[0m\n",
            "\u001b[32mPerson 2: Thank you for using our services.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJWkQI_8p-j"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrnQ9eNV8p-k",
        "outputId": "48a571f6-89a7-4806-bfb0-218f60db9f8a"
      },
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "untokenized_data = []\n",
        "\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    untokenized_data.append(result)\n",
        "\n",
        "print(untokenized_data[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HYge3h3ehpf"
      },
      "source": [
        "Splitting the list to a train and eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buE0b8bjx_p_",
        "outputId": "48bd5089-e651-4f30-b06c-752520be3aff"
      },
      "source": [
        "random.shuffle(untokenized_data)\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47DVWO7Sehpg"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## Creating input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUbgsIdehpg"
      },
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)\n",
        "        yield (d, d)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS5nbUXzehpg"
      },
      "source": [
        "Let's define our data pipeline for tokenizing and batching our data. We will also filter by maxlen and use bucketing for batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZgK5FAAWwOu"
      },
      "source": [
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE),\n",
        "    trax.data.FilterByLength(2048),\n",
        "    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\n",
        "                             batch_sizes=[1024, 512, 256, 128, 64]),\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwFEbYuiYqbo"
      },
      "source": [
        "Peek into the train stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iBQEvhLYRot",
        "outputId": "d9f21d26-7deb-4f4a-c2b8-5a7200cd6544"
      },
      "source": [
        "# the stream generators will yield (input, target, mask_weights).\n",
        "inp, _, _ = next(train_stream)\n",
        "print(\"input shape: \", inp.shape)\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (64, 512)\n",
            " Person 1: I'm looking for places to eat in the North part of town. Person 2: its called city stop restaurant, serves european food and address is Cambridge City Football Club Milton Road Chesterton Person 1: Is it a cheap restaurant? Person 2: No, it is expensive. Person 1: I need a cheap place, please. Person 2: Royal spice is cheap and looks great. Person 1: Thanks, will you please book a table for 6 people on saturday at 12:45? Person 2: Booking was successful. The table will be reserved for 15 minutes.\n",
            "Reference number is : Q91F26L3. Person 1: Perfect! Thank you for all of your help. Person 2: You're more than welcome. May I do anything else for you today? Person 1: Wait, I might want to change my mind about that restaurant. Are there any that serve food from Corsica? Person 2: No, there are none fitting that description, sir. Sorry about that. Person 1: I figured, my wife asks me odd random question sometimes, had to check.   I am all set, thanks. Person 2: Do you need any further assistance? Person 1: No, thank you. No further questions for now. Goodbye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsfaBEyd4Ks4"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "# 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidbAcoR6duP"
      },
      "source": [
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = trax.models.reformer.ReformerLM(\n",
        "        vocab_size=vocab_size,\n",
        "        n_layers=n_layers,\n",
        "        mode=mode,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKRTLXAnehpi",
        "outputId": "c46e5735-011b-4e2e-d554-3ffda2613a66"
      },
      "source": [
        "temp_model = ReformerLM(mode='train')\n",
        "print(str(temp_model))\n",
        "\n",
        "del temp_model "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_33000\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQehGhoD4Psl"
      },
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, n_layers=2, output_dir = \"./model/\"):\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.008)\n",
        "    \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_gen,\n",
        "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "        n_steps_per_checkpoint=50\n",
        "    )\n",
        "    \n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=eval_gen,\n",
        "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()]\n",
        "    )\n",
        "    \n",
        "    loop = training.Loop(model=ReformerLM(n_layers=n_layers),\n",
        "                         tasks=[train_task],\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdE7ie71aXxl"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9jERXY46I6J",
        "outputId": "98d619f5-9e55-4441-9560-481e9a64a1e4"
      },
      "source": [
        "if LOAD_MODEL == False:\n",
        "  !rm -f model/model.pkl.gz\n",
        "  loop = training_loop(ReformerLM, train_stream, eval_stream, n_layers=N_LAYERS)\n",
        "else:\n",
        "  loop = training_loop(ReformerLM, train_stream, eval_stream, n_layers=N_LAYERS)\n",
        "  loop.model.init_from_file('model/model.pkl.gz')\n",
        "\n",
        "if TRAIN == True:\n",
        "  loop.run(TRAIN_STEPS)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 70673640\n",
            "Step      1: Ran 1 train steps in 110.10 secs\n",
            "Step      1: train WeightedCategoryCrossEntropy |  10.41821575\n",
            "Step      1: eval  WeightedCategoryCrossEntropy |  10.40265846\n",
            "Step      1: eval      WeightedCategoryAccuracy |  0.00001045\n",
            "\n",
            "Step     50: Ran 49 train steps in 285.97 secs\n",
            "Step     50: train WeightedCategoryCrossEntropy |  7.41175556\n",
            "Step     50: eval  WeightedCategoryCrossEntropy |  5.57908297\n",
            "Step     50: eval      WeightedCategoryAccuracy |  0.06284281\n",
            "\n",
            "Step    100: Ran 50 train steps in 142.69 secs\n",
            "Step    100: train WeightedCategoryCrossEntropy |  5.55420208\n",
            "Step    100: eval  WeightedCategoryCrossEntropy |  5.57832050\n",
            "Step    100: eval      WeightedCategoryAccuracy |  0.06384477\n",
            "\n",
            "Step    150: Ran 50 train steps in 137.62 secs\n",
            "Step    150: train WeightedCategoryCrossEntropy |  5.42753839\n",
            "Step    150: eval  WeightedCategoryCrossEntropy |  5.08133507\n",
            "Step    150: eval      WeightedCategoryAccuracy |  0.14760922\n",
            "\n",
            "Step    200: Ran 50 train steps in 138.74 secs\n",
            "Step    200: train WeightedCategoryCrossEntropy |  4.69271612\n",
            "Step    200: eval  WeightedCategoryCrossEntropy |  4.15966845\n",
            "Step    200: eval      WeightedCategoryAccuracy |  0.26208997\n",
            "\n",
            "Step    250: Ran 50 train steps in 136.16 secs\n",
            "Step    250: train WeightedCategoryCrossEntropy |  3.78454280\n",
            "Step    250: eval  WeightedCategoryCrossEntropy |  3.42808342\n",
            "Step    250: eval      WeightedCategoryAccuracy |  0.33366424\n",
            "\n",
            "Step    300: Ran 50 train steps in 143.27 secs\n",
            "Step    300: train WeightedCategoryCrossEntropy |  3.30961752\n",
            "Step    300: eval  WeightedCategoryCrossEntropy |  3.11169767\n",
            "Step    300: eval      WeightedCategoryAccuracy |  0.37165266\n",
            "\n",
            "Step    350: Ran 50 train steps in 146.18 secs\n",
            "Step    350: train WeightedCategoryCrossEntropy |  3.00374103\n",
            "Step    350: eval  WeightedCategoryCrossEntropy |  2.83163357\n",
            "Step    350: eval      WeightedCategoryAccuracy |  0.40868267\n",
            "\n",
            "Step    400: Ran 50 train steps in 140.78 secs\n",
            "Step    400: train WeightedCategoryCrossEntropy |  2.78563857\n",
            "Step    400: eval  WeightedCategoryCrossEntropy |  2.90250921\n",
            "Step    400: eval      WeightedCategoryAccuracy |  0.39511275\n",
            "\n",
            "Step    450: Ran 50 train steps in 137.79 secs\n",
            "Step    450: train WeightedCategoryCrossEntropy |  2.68790865\n",
            "Step    450: eval  WeightedCategoryCrossEntropy |  2.58988929\n",
            "Step    450: eval      WeightedCategoryAccuracy |  0.44366863\n",
            "\n",
            "Step    500: Ran 50 train steps in 140.08 secs\n",
            "Step    500: train WeightedCategoryCrossEntropy |  2.39728856\n",
            "Step    500: eval  WeightedCategoryCrossEntropy |  2.46912146\n",
            "Step    500: eval      WeightedCategoryAccuracy |  0.46836656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztAxOfL3CXsd",
        "outputId": "8a3658e0-b43e-46b4-dd06-469b09e98746"
      },
      "source": [
        "loop.run(100)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step    550: Ran 50 train steps in 144.24 secs\n",
            "Step    550: train WeightedCategoryCrossEntropy |  2.26461148\n",
            "Step    550: eval  WeightedCategoryCrossEntropy |  2.14184332\n",
            "Step    550: eval      WeightedCategoryAccuracy |  0.53136408\n",
            "\n",
            "Step    600: Ran 50 train steps in 139.03 secs\n",
            "Step    600: train WeightedCategoryCrossEntropy |  2.10417056\n",
            "Step    600: eval  WeightedCategoryCrossEntropy |  2.11950707\n",
            "Step    600: eval      WeightedCategoryAccuracy |  0.52626514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SMJ2KKuFIHL",
        "outputId": "193be083-42c3-4c9d-b36c-fcd468eb5a35"
      },
      "source": [
        "loop.run(100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step    650: Ran 50 train steps in 132.58 secs\n",
            "Step    650: train WeightedCategoryCrossEntropy |  1.98943090\n",
            "Step    650: eval  WeightedCategoryCrossEntropy |  1.98838365\n",
            "Step    650: eval      WeightedCategoryAccuracy |  0.54585487\n",
            "\n",
            "Step    700: Ran 50 train steps in 139.27 secs\n",
            "Step    700: train WeightedCategoryCrossEntropy |  1.87878287\n",
            "Step    700: eval  WeightedCategoryCrossEntropy |  1.88107276\n",
            "Step    700: eval      WeightedCategoryAccuracy |  0.56508166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAGlkMZW6rKn"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "# 4. Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48rWH-Pehpk"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    # number of input positions to remember in a cache when doing fast inference. \n",
        "    kwargs['predict_mem_len'] = 1024\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\n",
        "    kwargs['predict_drop_len'] = 128\n",
        "    # return the attention layer with the parameters defined above\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# Getting the model with new attention for prediction\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=N_LAYERS,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si_Xo4nPehpk"
      },
      "source": [
        "# TRAX needs the model to be initialized with this shape\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "model.init(shape11)\n",
        "\n",
        "# Loading weights from the trained model\n",
        "model.weights = loop.eval_model.weights\n",
        "\n",
        "# saving the starting state for each new dialogue prediction\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdo9Fziy-8ZW",
        "outputId": "5388527b-50f0-420a-c687-bec6ed01dc7c"
      },
      "source": [
        "str(model) == str(loop.eval_model)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB057xZMehpk"
      },
      "source": [
        "Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnBKD1xehpk"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNw33qRBehpl"
      },
      "source": [
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    input_tokens_with_batch = input_tokens[None]\n",
        "    \n",
        "    # Using the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        model=ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7Xr4Dmehpm"
      },
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    sentence = ''\n",
        "    counter = 0\n",
        "    \n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    print(colored(start_sentence.split(delimiter_2)[0].strip(), 'green'))\n",
        "    \n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(colored(f'{delimiter_2}{sentence}', 'red'))\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(colored(f'{delimiter_1}{sentence}', 'green'))\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    \n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZNfs6Zehpm",
        "outputId": "af3c6155-6087-43a2-bebe-9903aa10b5d0"
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mPerson 1: Are there theatres in town?\u001b[0m\n",
            "\u001b[31mPerson 2: : There are 13 attractions in the centre of town. Do you have a preference? \u001b[0m\n",
            "\u001b[32mPerson 1: I'd like to go to go to go to go to go to go to go to go to go to go to go to go. \u001b[0m\n",
            "\u001b[31mPerson 2: There are many options for you. Is there a specific area you would like to visit? \u001b[0m\n",
            "\u001b[32mPerson 1: I'd like to go to go to go to go to the theatre. \u001b[0m\n",
            "\u001b[31mPerson 2: There are many museums in the centre. Do you have a preference? \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTXOnhJEehpm",
        "outputId": "87e76c02-062b-4485-f4b0-feb532cec9d8"
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mPerson 1: Is there a hospital nearby?\u001b[0m\n",
            "\u001b[31mPerson 2: : The address is Hills Rd, Cambridge, Cambridge, Cambridge, Cambridge, Cambridge, Cambridge, CB20QQ. Is there anything else I can help you with? \u001b[0m\n",
            "\u001b[32mPerson 1: No, I'm looking for a train leaving on Saturday. \u001b[0m\n",
            "\u001b[31mPerson 2: There are 202 trains leaving on Friday on Friday. Where will you be departing from? \u001b[0m\n",
            "\u001b[32mPerson 1: I'm leaving from Cambridge and going to Cambridge on Monday. \u001b[0m\n",
            "\u001b[31mPerson 2: There are 202 trains leaving at 05:17. Would you like me to book a ticket? \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wor46vEGehpm",
        "outputId": "86a583ee-0a08-476b-dc4e-6cd4174f8ad2"
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mPerson 1: Can you book a taxi?\u001b[0m\n",
            "\u001b[31mPerson 2: : Sure! Where are you departing from? \u001b[0m\n",
            "\u001b[32mPerson 1: I'm going to go to Cambridge. \u001b[0m\n",
            "\u001b[31mPerson 2: I can help narrow down with that. Where are you departing from? \u001b[0m\n",
            "\u001b[32mPerson 1: I'd like to leave after 15:15. \u001b[0m\n",
            "\u001b[31mPerson 2: I have a yellow volkswagen. The contact number is 076749756. \u001b[0m\n",
            "\u001b[32mPerson 1: Thank you so much for your help. \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}